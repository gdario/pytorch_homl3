{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfe6779",
   "metadata": {},
   "source": [
    "# Chapter 10\n",
    "\n",
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8601b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76b9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'data'\n",
    "full_train_dset = FashionMNIST(root=root_dir,\n",
    "                               train=True,\n",
    "                               download=True,\n",
    "                               transform=ToTensor())\n",
    "test_dset = FashionMNIST(root=root_dir,\n",
    "                         train=False,\n",
    "                         download=True,\n",
    "                         transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d30ae606",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "if torch.cuda.device_count() == 2:\n",
    "    torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4c46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = Subset(full_train_dset, indices=range(55000))\n",
    "valid_dset = Subset(full_train_dset, indices=range(55000, 60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39bf7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dset), len(valid_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f761d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Default batch size on keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef43a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5f4d3",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "In Geron's book the data are normalized by dividing by 255. The `ToTensor()` transform added to the Dataset definition takes care of this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c94788",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, tgt = next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788b4c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd6fe9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02309bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential_model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=784, out_features=300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=300, out_features=100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features=10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sequential_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c057db0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (sequential_model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model = MyModel()\n",
    "classif_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5596b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0237, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.sequential_model[1].bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22176dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0046, -0.0107, -0.0128, -0.0025,  0.0348, -0.0174,  0.0259, -0.0163,\n",
       "         0.0243, -0.0059], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.sequential_model[1].weight[0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce2bc7",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "\n",
    "In Keras weights in a dense layer are, [by default](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), initialized with a Glorot Uniform initialization.\n",
    "\n",
    "In Pytorch weights in a linear layer are, [by default](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) initialized with a uniform distribution with  U(âˆ’k,k) where k=1/in_features. This is essentially a LeCun uniform initialization. Note that in PyTorch, Glorot initialization is called Xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9437ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf40d666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (sequential_model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b19df074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.sequential_model[1].bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cecb27f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0482,  0.0128,  0.0131, -0.0169, -0.0428,  0.0225,  0.0709,  0.0432,\n",
       "        -0.0656, -0.0359], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.sequential_model[1].weight[0, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f506245",
   "metadata": {},
   "source": [
    "Both the bias and the weight look different. We assume that we are using the same initialization approach as Keras.\n",
    "\n",
    "The Keras model has a final softmax activation. If we use PyTorch cross-entropy loss, the softmax is fused in the loss, so we don't need to include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74120b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (sequential_model): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d87bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b844b7f",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "The example in Geron's book uses the \"sgd\" optimizer, which I suspect corresponds to the [default settings](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD), i.e., lr = 0.01 and momentum = 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7487429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif_optim = optim.SGD(classif_model.parameters(), lr=0.01, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e26dc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x)\n",
    "        batch_loss = loss_fn(preds, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 250 == 0:\n",
    "            print(f'Train loss: {batch_loss.item():>.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67df4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x)\n",
    "            total_loss += loss_fn(preds, y).item()\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "    avg_batch_loss = total_loss / num_batches\n",
    "    accuracy = correct / num_obs\n",
    "    print('Validation:')\n",
    "    print(f'\\nAverage loss: {avg_batch_loss:>.5} - Accuracy: {accuracy:>.3}')\n",
    "    return avg_batch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffd85440",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aef7cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch: 1 -----\n",
      "\n",
      "Train loss: 2.3929\n",
      "Train loss: 0.93195\n",
      "Train loss: 0.54905\n",
      "Train loss: 0.60388\n",
      "Train loss: 0.77711\n",
      "Train loss: 0.56643\n",
      "Train loss: 0.43589\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.54566 - Accuracy: 0.806\n",
      "\n",
      "----- Epoch: 2 -----\n",
      "\n",
      "Train loss: 0.50713\n",
      "Train loss: 0.82873\n",
      "Train loss: 0.56208\n",
      "Train loss: 0.5899\n",
      "Train loss: 0.3201\n",
      "Train loss: 0.49011\n",
      "Train loss: 0.341\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.44484 - Accuracy: 0.84\n",
      "\n",
      "----- Epoch: 3 -----\n",
      "\n",
      "Train loss: 0.6944\n",
      "Train loss: 0.22429\n",
      "Train loss: 0.39264\n",
      "Train loss: 0.5807\n",
      "Train loss: 0.36953\n",
      "Train loss: 0.42035\n",
      "Train loss: 0.45779\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.45213 - Accuracy: 0.838\n",
      "\n",
      "----- Epoch: 4 -----\n",
      "\n",
      "Train loss: 0.55071\n",
      "Train loss: 0.34373\n",
      "Train loss: 0.4501\n",
      "Train loss: 0.38788\n",
      "Train loss: 0.65659\n",
      "Train loss: 0.46598\n",
      "Train loss: 0.36876\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.40774 - Accuracy: 0.852\n",
      "\n",
      "----- Epoch: 5 -----\n",
      "\n",
      "Train loss: 0.17387\n",
      "Train loss: 0.26334\n",
      "Train loss: 0.39497\n",
      "Train loss: 0.30913\n",
      "Train loss: 0.37235\n",
      "Train loss: 0.2081\n",
      "Train loss: 0.41848\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.40972 - Accuracy: 0.848\n",
      "\n",
      "----- Epoch: 6 -----\n",
      "\n",
      "Train loss: 0.21791\n",
      "Train loss: 0.51094\n",
      "Train loss: 0.33523\n",
      "Train loss: 0.2533\n",
      "Train loss: 0.19382\n",
      "Train loss: 0.2622\n",
      "Train loss: 0.46822\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.39369 - Accuracy: 0.861\n",
      "\n",
      "----- Epoch: 7 -----\n",
      "\n",
      "Train loss: 0.40722\n",
      "Train loss: 0.26955\n",
      "Train loss: 0.34359\n",
      "Train loss: 0.28786\n",
      "Train loss: 0.24772\n",
      "Train loss: 0.24408\n",
      "Train loss: 0.47527\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.37395 - Accuracy: 0.868\n",
      "\n",
      "----- Epoch: 8 -----\n",
      "\n",
      "Train loss: 0.30651\n",
      "Train loss: 0.17811\n",
      "Train loss: 0.63752\n",
      "Train loss: 0.2912\n",
      "Train loss: 0.2058\n",
      "Train loss: 0.45292\n",
      "Train loss: 0.29453\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.3685 - Accuracy: 0.864\n",
      "\n",
      "----- Epoch: 9 -----\n",
      "\n",
      "Train loss: 0.22157\n",
      "Train loss: 0.44828\n",
      "Train loss: 0.35175\n",
      "Train loss: 0.081123\n",
      "Train loss: 0.21991\n",
      "Train loss: 0.35447\n",
      "Train loss: 0.29572\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.36027 - Accuracy: 0.868\n",
      "\n",
      "----- Epoch: 10 -----\n",
      "\n",
      "Train loss: 0.37674\n",
      "Train loss: 0.21521\n",
      "Train loss: 0.32373\n",
      "Train loss: 0.37809\n",
      "Train loss: 0.41976\n",
      "Train loss: 0.40832\n",
      "Train loss: 0.34942\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.35003 - Accuracy: 0.873\n",
      "\n",
      "----- Epoch: 11 -----\n",
      "\n",
      "Train loss: 0.34122\n",
      "Train loss: 0.26116\n",
      "Train loss: 0.28198\n",
      "Train loss: 0.17404\n",
      "Train loss: 0.40024\n",
      "Train loss: 0.33156\n",
      "Train loss: 0.3197\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.36324 - Accuracy: 0.869\n",
      "\n",
      "----- Epoch: 12 -----\n",
      "\n",
      "Train loss: 0.31507\n",
      "Train loss: 0.2939\n",
      "Train loss: 0.527\n",
      "Train loss: 0.49089\n",
      "Train loss: 0.29493\n",
      "Train loss: 0.39149\n",
      "Train loss: 0.26023\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.34914 - Accuracy: 0.873\n",
      "\n",
      "----- Epoch: 13 -----\n",
      "\n",
      "Train loss: 0.23672\n",
      "Train loss: 0.59113\n",
      "Train loss: 0.20566\n",
      "Train loss: 0.3276\n",
      "Train loss: 0.24932\n",
      "Train loss: 0.34008\n",
      "Train loss: 0.5186\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33834 - Accuracy: 0.878\n",
      "\n",
      "----- Epoch: 14 -----\n",
      "\n",
      "Train loss: 0.44144\n",
      "Train loss: 0.32915\n",
      "Train loss: 0.32898\n",
      "Train loss: 0.52876\n",
      "Train loss: 0.23848\n",
      "Train loss: 0.47077\n",
      "Train loss: 0.25026\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.34504 - Accuracy: 0.868\n",
      "\n",
      "----- Epoch: 15 -----\n",
      "\n",
      "Train loss: 0.074151\n",
      "Train loss: 0.35008\n",
      "Train loss: 0.31019\n",
      "Train loss: 0.26952\n",
      "Train loss: 0.25351\n",
      "Train loss: 0.15822\n",
      "Train loss: 0.12385\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.3363 - Accuracy: 0.878\n",
      "\n",
      "----- Epoch: 16 -----\n",
      "\n",
      "Train loss: 0.15715\n",
      "Train loss: 0.40975\n",
      "Train loss: 0.33136\n",
      "Train loss: 0.24438\n",
      "Train loss: 0.26765\n",
      "Train loss: 0.094521\n",
      "Train loss: 0.19202\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.32644 - Accuracy: 0.879\n",
      "\n",
      "----- Epoch: 17 -----\n",
      "\n",
      "Train loss: 0.24729\n",
      "Train loss: 0.13366\n",
      "Train loss: 0.55091\n",
      "Train loss: 0.15587\n",
      "Train loss: 0.29587\n",
      "Train loss: 0.80541\n",
      "Train loss: 0.19653\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33028 - Accuracy: 0.878\n",
      "\n",
      "----- Epoch: 18 -----\n",
      "\n",
      "Train loss: 0.24042\n",
      "Train loss: 0.25368\n",
      "Train loss: 0.14729\n",
      "Train loss: 0.28522\n",
      "Train loss: 0.23599\n",
      "Train loss: 0.232\n",
      "Train loss: 0.079904\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.31998 - Accuracy: 0.879\n",
      "\n",
      "----- Epoch: 19 -----\n",
      "\n",
      "Train loss: 0.31942\n",
      "Train loss: 0.15915\n",
      "Train loss: 0.13229\n",
      "Train loss: 0.43348\n",
      "Train loss: 0.29048\n",
      "Train loss: 0.32318\n",
      "Train loss: 0.14364\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33246 - Accuracy: 0.879\n",
      "\n",
      "----- Epoch: 20 -----\n",
      "\n",
      "Train loss: 0.22816\n",
      "Train loss: 0.254\n",
      "Train loss: 0.20144\n",
      "Train loss: 0.26267\n",
      "Train loss: 0.23758\n",
      "Train loss: 0.30405\n",
      "Train loss: 0.17193\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.32255 - Accuracy: 0.884\n",
      "\n",
      "----- Epoch: 21 -----\n",
      "\n",
      "Train loss: 0.48069\n",
      "Train loss: 0.28882\n",
      "Train loss: 0.21534\n",
      "Train loss: 0.41156\n",
      "Train loss: 0.23807\n",
      "Train loss: 0.32192\n",
      "Train loss: 0.40074\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33132 - Accuracy: 0.882\n",
      "\n",
      "----- Epoch: 22 -----\n",
      "\n",
      "Train loss: 0.18732\n",
      "Train loss: 0.25047\n",
      "Train loss: 0.10922\n",
      "Train loss: 0.36437\n",
      "Train loss: 0.38701\n",
      "Train loss: 0.49191\n",
      "Train loss: 0.22256\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33125 - Accuracy: 0.885\n",
      "\n",
      "----- Epoch: 23 -----\n",
      "\n",
      "Train loss: 0.14453\n",
      "Train loss: 0.078922\n",
      "Train loss: 0.3022\n",
      "Train loss: 0.29497\n",
      "Train loss: 0.60459\n",
      "Train loss: 0.32232\n",
      "Train loss: 0.34441\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.31403 - Accuracy: 0.886\n",
      "\n",
      "----- Epoch: 24 -----\n",
      "\n",
      "Train loss: 0.25234\n",
      "Train loss: 0.13552\n",
      "Train loss: 0.28284\n",
      "Train loss: 0.5576\n",
      "Train loss: 0.40316\n",
      "Train loss: 0.16364\n",
      "Train loss: 0.38552\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.32869 - Accuracy: 0.879\n",
      "\n",
      "----- Epoch: 25 -----\n",
      "\n",
      "Train loss: 0.34637\n",
      "Train loss: 0.23208\n",
      "Train loss: 0.37245\n",
      "Train loss: 0.40651\n",
      "Train loss: 0.11615\n",
      "Train loss: 0.27055\n",
      "Train loss: 0.11838\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33182 - Accuracy: 0.882\n",
      "\n",
      "----- Epoch: 26 -----\n",
      "\n",
      "Train loss: 0.23223\n",
      "Train loss: 0.21325\n",
      "Train loss: 0.35671\n",
      "Train loss: 0.21425\n",
      "Train loss: 0.088785\n",
      "Train loss: 0.10147\n",
      "Train loss: 0.55444\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.31769 - Accuracy: 0.885\n",
      "\n",
      "----- Epoch: 27 -----\n",
      "\n",
      "Train loss: 0.29859\n",
      "Train loss: 0.21891\n",
      "Train loss: 0.34427\n",
      "Train loss: 0.23988\n",
      "Train loss: 0.21444\n",
      "Train loss: 0.10038\n",
      "Train loss: 0.61906\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.31877 - Accuracy: 0.886\n",
      "\n",
      "----- Epoch: 28 -----\n",
      "\n",
      "Train loss: 0.077503\n",
      "Train loss: 0.19539\n",
      "Train loss: 0.30494\n",
      "Train loss: 0.10375\n",
      "Train loss: 0.50426\n",
      "Train loss: 0.1793\n",
      "Train loss: 0.19484\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.32725 - Accuracy: 0.876\n",
      "\n",
      "----- Epoch: 29 -----\n",
      "\n",
      "Train loss: 0.36737\n",
      "Train loss: 0.48091\n",
      "Train loss: 0.29096\n",
      "Train loss: 0.40817\n",
      "Train loss: 0.24323\n",
      "Train loss: 0.27825\n",
      "Train loss: 0.23089\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.3158 - Accuracy: 0.884\n",
      "\n",
      "----- Epoch: 30 -----\n",
      "\n",
      "Train loss: 0.28235\n",
      "Train loss: 0.10454\n",
      "Train loss: 0.17486\n",
      "Train loss: 0.065819\n",
      "Train loss: 0.10901\n",
      "Train loss: 0.33508\n",
      "Train loss: 0.32242\n",
      "Validation:\n",
      "\n",
      "Average loss: 0.33045 - Accuracy: 0.879\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n----- Epoch: {epoch+1} -----\\n')\n",
    "    train(train_loader, classif_model, loss_fn, classif_optim)\n",
    "    loss, acc = validate(valid_loader, classif_model, loss_fn)\n",
    "    test_loss.append(loss)\n",
    "    test_accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe0520",
   "metadata": {},
   "source": [
    "This is very similar to the performance shown in Geron's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8bcfcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(30), test_accuracy, 'r-', label='Validation accuracy')\n",
    "plt.plot(np.arange(30), test_loss, 'b-', label='Validation loss')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa42855",
   "metadata": {},
   "source": [
    "### Performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "246cc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_performance(dataloader, model):\n",
    "    test_accuracy = 0\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x)\n",
    "            correct += (preds.argmax(1) == y).type(torch.float32).sum().item()\n",
    "    return correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c13e9632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8774"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_performance(test_loader, classif_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df76ea",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API\n",
    "\n",
    "For this example we use the California Housing dataset. After creating a training, validation, and test set, we must normalize the data and organize them into dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3999407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25dd2e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11610, 8), (3870, 8), (5160, 8))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdde54a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.68120000e+00,  2.50000000e+01,  4.19220056e+00,  1.02228412e+00,\n",
       "        1.39200000e+03,  3.87743733e+00,  3.60600000e+01, -1.19010000e+02])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04183e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), (8,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, std_devs = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "means.shape, std_devs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b396fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = ((x_train - means) / std_devs).astype(np.float32)\n",
    "x_valid = ((x_valid - means) / std_devs).astype(np.float32)\n",
    "x_test = ((x_test - means) / std_devs).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "411e407f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84bd8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor_train = torch.FloatTensor(x_train)\n",
    "y_tensor_train = torch.FloatTensor(y_train).unsqueeze(-1)\n",
    "x_tensor_valid = torch.FloatTensor(x_valid)\n",
    "y_tensor_valid = torch.FloatTensor(y_valid).unsqueeze(-1)\n",
    "x_tensor_test = torch.FloatTensor(x_test)\n",
    "y_tensor_test = torch.FloatTensor(y_test).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a75419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = TensorDataset(x_tensor_train, y_tensor_train)\n",
    "valid_dset = TensorDataset(x_tensor_valid, y_tensor_valid)\n",
    "test_dset = TensorDataset(x_tensor_test, y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e522ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3852c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential_model = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=50, out_features=50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=50, out_features=50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=50, out_features=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sequential_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78749533",
   "metadata": {},
   "source": [
    "### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52aa1245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionModel(\n",
       "  (sequential_model): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model = RegressionModel()\n",
    "regression_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7727e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = regression_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d8c444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(dataloader, model, loss_fn, optimizer):\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x)\n",
    "        batch_loss = loss_fn(preds, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Train Loss: {batch_loss.item():>.5}. Batch: [{batch}/{num_batches}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96cbfc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression(dataloader, model, loss_fn):\n",
    "    total_loss = 0\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x)\n",
    "            total_loss += loss_fn(preds, y).item()\n",
    "        print(f'Validation Loss: {total_loss/num_batches:>.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd0006c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "regression_optim = optim.Adam(regression_model.parameters(), lr=1e-3, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3a1855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----- Epoch 1 -----\n",
      "\n",
      "Train Loss: 5.6102. Batch: [0/363]\n",
      "Train Loss: 0.52433. Batch: [100/363]\n",
      "Train Loss: 0.66967. Batch: [200/363]\n",
      "Train Loss: 0.33699. Batch: [300/363]\n",
      "Validation Loss: 0.72447\n",
      "\n",
      " ----- Epoch 2 -----\n",
      "\n",
      "Train Loss: 0.5272. Batch: [0/363]\n",
      "Train Loss: 0.22577. Batch: [100/363]\n",
      "Train Loss: 0.23623. Batch: [200/363]\n",
      "Train Loss: 0.3455. Batch: [300/363]\n",
      "Validation Loss: 3.0015\n",
      "\n",
      " ----- Epoch 3 -----\n",
      "\n",
      "Train Loss: 0.50808. Batch: [0/363]\n",
      "Train Loss: 0.91866. Batch: [100/363]\n",
      "Train Loss: 0.68902. Batch: [200/363]\n",
      "Train Loss: 0.16467. Batch: [300/363]\n",
      "Validation Loss: 1.1817\n",
      "\n",
      " ----- Epoch 4 -----\n",
      "\n",
      "Train Loss: 0.29163. Batch: [0/363]\n",
      "Train Loss: 0.23263. Batch: [100/363]\n",
      "Train Loss: 0.3162. Batch: [200/363]\n",
      "Train Loss: 0.42507. Batch: [300/363]\n",
      "Validation Loss: 1.0185\n",
      "\n",
      " ----- Epoch 5 -----\n",
      "\n",
      "Train Loss: 0.16569. Batch: [0/363]\n",
      "Train Loss: 0.12829. Batch: [100/363]\n",
      "Train Loss: 0.53072. Batch: [200/363]\n",
      "Train Loss: 0.28417. Batch: [300/363]\n",
      "Validation Loss: 0.91683\n",
      "\n",
      " ----- Epoch 6 -----\n",
      "\n",
      "Train Loss: 0.57888. Batch: [0/363]\n",
      "Train Loss: 0.42448. Batch: [100/363]\n",
      "Train Loss: 0.37579. Batch: [200/363]\n",
      "Train Loss: 0.42015. Batch: [300/363]\n",
      "Validation Loss: 0.81377\n",
      "\n",
      " ----- Epoch 7 -----\n",
      "\n",
      "Train Loss: 0.21832. Batch: [0/363]\n",
      "Train Loss: 0.20163. Batch: [100/363]\n",
      "Train Loss: 0.30927. Batch: [200/363]\n",
      "Train Loss: 0.35572. Batch: [300/363]\n",
      "Validation Loss: 0.59703\n",
      "\n",
      " ----- Epoch 8 -----\n",
      "\n",
      "Train Loss: 0.2511. Batch: [0/363]\n",
      "Train Loss: 0.46713. Batch: [100/363]\n",
      "Train Loss: 0.13724. Batch: [200/363]\n",
      "Train Loss: 0.16326. Batch: [300/363]\n",
      "Validation Loss: 1.3477\n",
      "\n",
      " ----- Epoch 9 -----\n",
      "\n",
      "Train Loss: 0.24742. Batch: [0/363]\n",
      "Train Loss: 0.45321. Batch: [100/363]\n",
      "Train Loss: 0.26279. Batch: [200/363]\n",
      "Train Loss: 0.36759. Batch: [300/363]\n",
      "Validation Loss: 0.46956\n",
      "\n",
      " ----- Epoch 10 -----\n",
      "\n",
      "Train Loss: 0.26472. Batch: [0/363]\n",
      "Train Loss: 0.29823. Batch: [100/363]\n",
      "Train Loss: 0.37572. Batch: [200/363]\n",
      "Train Loss: 0.37124. Batch: [300/363]\n",
      "Validation Loss: 0.53035\n",
      "\n",
      " ----- Epoch 11 -----\n",
      "\n",
      "Train Loss: 0.49565. Batch: [0/363]\n",
      "Train Loss: 0.17724. Batch: [100/363]\n",
      "Train Loss: 0.471. Batch: [200/363]\n",
      "Train Loss: 0.39861. Batch: [300/363]\n",
      "Validation Loss: 0.30215\n",
      "\n",
      " ----- Epoch 12 -----\n",
      "\n",
      "Train Loss: 0.22916. Batch: [0/363]\n",
      "Train Loss: 0.30264. Batch: [100/363]\n",
      "Train Loss: 0.22558. Batch: [200/363]\n",
      "Train Loss: 0.58944. Batch: [300/363]\n",
      "Validation Loss: 0.75346\n",
      "\n",
      " ----- Epoch 13 -----\n",
      "\n",
      "Train Loss: 0.39617. Batch: [0/363]\n",
      "Train Loss: 0.18391. Batch: [100/363]\n",
      "Train Loss: 0.19027. Batch: [200/363]\n",
      "Train Loss: 0.15518. Batch: [300/363]\n",
      "Validation Loss: 0.36918\n",
      "\n",
      " ----- Epoch 14 -----\n",
      "\n",
      "Train Loss: 0.095983. Batch: [0/363]\n",
      "Train Loss: 0.30341. Batch: [100/363]\n",
      "Train Loss: 0.30225. Batch: [200/363]\n",
      "Train Loss: 0.49996. Batch: [300/363]\n",
      "Validation Loss: 0.65933\n",
      "\n",
      " ----- Epoch 15 -----\n",
      "\n",
      "Train Loss: 0.30257. Batch: [0/363]\n",
      "Train Loss: 0.2223. Batch: [100/363]\n",
      "Train Loss: 0.3018. Batch: [200/363]\n",
      "Train Loss: 0.27683. Batch: [300/363]\n",
      "Validation Loss: 0.31489\n",
      "\n",
      " ----- Epoch 16 -----\n",
      "\n",
      "Train Loss: 0.41517. Batch: [0/363]\n",
      "Train Loss: 0.34994. Batch: [100/363]\n",
      "Train Loss: 0.12178. Batch: [200/363]\n",
      "Train Loss: 0.26904. Batch: [300/363]\n",
      "Validation Loss: 0.28371\n",
      "\n",
      " ----- Epoch 17 -----\n",
      "\n",
      "Train Loss: 0.24527. Batch: [0/363]\n",
      "Train Loss: 0.24451. Batch: [100/363]\n",
      "Train Loss: 0.26126. Batch: [200/363]\n",
      "Train Loss: 0.31001. Batch: [300/363]\n",
      "Validation Loss: 0.36261\n",
      "\n",
      " ----- Epoch 18 -----\n",
      "\n",
      "Train Loss: 0.29418. Batch: [0/363]\n",
      "Train Loss: 0.28861. Batch: [100/363]\n",
      "Train Loss: 0.27387. Batch: [200/363]\n",
      "Train Loss: 0.2367. Batch: [300/363]\n",
      "Validation Loss: 0.32617\n",
      "\n",
      " ----- Epoch 19 -----\n",
      "\n",
      "Train Loss: 0.17428. Batch: [0/363]\n",
      "Train Loss: 0.22547. Batch: [100/363]\n",
      "Train Loss: 0.19352. Batch: [200/363]\n",
      "Train Loss: 0.13024. Batch: [300/363]\n",
      "Validation Loss: 0.27432\n",
      "\n",
      " ----- Epoch 20 -----\n",
      "\n",
      "Train Loss: 0.40023. Batch: [0/363]\n",
      "Train Loss: 0.43165. Batch: [100/363]\n",
      "Train Loss: 0.27234. Batch: [200/363]\n",
      "Train Loss: 0.4032. Batch: [300/363]\n",
      "Validation Loss: 0.4225\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f'\\n ----- Epoch {epoch+1} -----\\n')\n",
    "    train_regression(train_loader, regression_model, loss_fn, regression_optim)\n",
    "    eval_regression(valid_loader, regression_model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c488a27",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d41f6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = nn.Linear(in_features=8, out_features=30)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layer2 = nn.Linear(in_features=30, out_features=30)\n",
    "        self.output_layer = nn.Linear(in_features=38, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.relu(self.hidden_layer1(x))\n",
    "        z = self.relu(self.hidden_layer2(z))\n",
    "        w = torch.cat((x, z), dim=-1)\n",
    "        return self.output_layer(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d3a2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "widedeep = WideAndDeep()\n",
    "widedeep.apply(init_weights)\n",
    "widedeep = widedeep.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90428cb",
   "metadata": {},
   "source": [
    "Note that we need to create another optimizer, as the existing one has been trained on a different architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cf1cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnd_optim = optim.Adam(widedeep.parameters(), lr=1e-3, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "445a9987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.9973. Batch: [0/363]\n",
      "Train Loss: 0.79704. Batch: [100/363]\n",
      "Train Loss: 0.84926. Batch: [200/363]\n",
      "Train Loss: 0.51205. Batch: [300/363]\n",
      "Validation Loss: 0.89775\n",
      "Train Loss: 0.35408. Batch: [0/363]\n",
      "Train Loss: 0.21947. Batch: [100/363]\n",
      "Train Loss: 0.5549. Batch: [200/363]\n",
      "Train Loss: 0.44046. Batch: [300/363]\n",
      "Validation Loss: 0.42174\n",
      "Train Loss: 0.39329. Batch: [0/363]\n",
      "Train Loss: 0.26698. Batch: [100/363]\n",
      "Train Loss: 0.30138. Batch: [200/363]\n",
      "Train Loss: 0.58435. Batch: [300/363]\n",
      "Validation Loss: 0.4753\n",
      "Train Loss: 0.31146. Batch: [0/363]\n",
      "Train Loss: 0.59773. Batch: [100/363]\n",
      "Train Loss: 0.567. Batch: [200/363]\n",
      "Train Loss: 0.23096. Batch: [300/363]\n",
      "Validation Loss: 0.41072\n",
      "Train Loss: 0.77299. Batch: [0/363]\n",
      "Train Loss: 0.21619. Batch: [100/363]\n",
      "Train Loss: 0.49299. Batch: [200/363]\n",
      "Train Loss: 0.74151. Batch: [300/363]\n",
      "Validation Loss: 1.6573\n",
      "Train Loss: 0.39667. Batch: [0/363]\n",
      "Train Loss: 0.28282. Batch: [100/363]\n",
      "Train Loss: 0.44575. Batch: [200/363]\n",
      "Train Loss: 0.17881. Batch: [300/363]\n",
      "Validation Loss: 0.71231\n",
      "Train Loss: 0.67535. Batch: [0/363]\n",
      "Train Loss: 0.36444. Batch: [100/363]\n",
      "Train Loss: 0.10429. Batch: [200/363]\n",
      "Train Loss: 0.53311. Batch: [300/363]\n",
      "Validation Loss: 0.7562\n",
      "Train Loss: 0.15473. Batch: [0/363]\n",
      "Train Loss: 0.32273. Batch: [100/363]\n",
      "Train Loss: 0.24915. Batch: [200/363]\n",
      "Train Loss: 0.2213. Batch: [300/363]\n",
      "Validation Loss: 0.32609\n",
      "Train Loss: 0.39397. Batch: [0/363]\n",
      "Train Loss: 0.33174. Batch: [100/363]\n",
      "Train Loss: 0.51599. Batch: [200/363]\n",
      "Train Loss: 0.27612. Batch: [300/363]\n",
      "Validation Loss: 0.38544\n",
      "Train Loss: 0.25293. Batch: [0/363]\n",
      "Train Loss: 0.28669. Batch: [100/363]\n",
      "Train Loss: 0.28131. Batch: [200/363]\n",
      "Train Loss: 0.1963. Batch: [300/363]\n",
      "Validation Loss: 0.30593\n",
      "Train Loss: 0.41784. Batch: [0/363]\n",
      "Train Loss: 0.37577. Batch: [100/363]\n",
      "Train Loss: 0.34038. Batch: [200/363]\n",
      "Train Loss: 0.22354. Batch: [300/363]\n",
      "Validation Loss: 0.33383\n",
      "Train Loss: 0.30389. Batch: [0/363]\n",
      "Train Loss: 0.37427. Batch: [100/363]\n",
      "Train Loss: 0.42312. Batch: [200/363]\n",
      "Train Loss: 0.2087. Batch: [300/363]\n",
      "Validation Loss: 0.57638\n",
      "Train Loss: 0.21738. Batch: [0/363]\n",
      "Train Loss: 0.24816. Batch: [100/363]\n",
      "Train Loss: 0.30075. Batch: [200/363]\n",
      "Train Loss: 0.38909. Batch: [300/363]\n",
      "Validation Loss: 0.66743\n",
      "Train Loss: 0.32409. Batch: [0/363]\n",
      "Train Loss: 0.20578. Batch: [100/363]\n",
      "Train Loss: 0.49436. Batch: [200/363]\n",
      "Train Loss: 0.39032. Batch: [300/363]\n",
      "Validation Loss: 1.5179\n",
      "Train Loss: 0.54557. Batch: [0/363]\n",
      "Train Loss: 0.39268. Batch: [100/363]\n",
      "Train Loss: 0.22877. Batch: [200/363]\n",
      "Train Loss: 0.20232. Batch: [300/363]\n",
      "Validation Loss: 0.58434\n",
      "Train Loss: 0.19381. Batch: [0/363]\n",
      "Train Loss: 0.21972. Batch: [100/363]\n",
      "Train Loss: 0.15141. Batch: [200/363]\n",
      "Train Loss: 0.37172. Batch: [300/363]\n",
      "Validation Loss: 1.1938\n",
      "Train Loss: 0.21848. Batch: [0/363]\n",
      "Train Loss: 0.31644. Batch: [100/363]\n",
      "Train Loss: 0.078398. Batch: [200/363]\n",
      "Train Loss: 0.31938. Batch: [300/363]\n",
      "Validation Loss: 1.2863\n",
      "Train Loss: 0.18768. Batch: [0/363]\n",
      "Train Loss: 0.36055. Batch: [100/363]\n",
      "Train Loss: 0.4308. Batch: [200/363]\n",
      "Train Loss: 0.11241. Batch: [300/363]\n",
      "Validation Loss: 0.97347\n",
      "Train Loss: 0.18495. Batch: [0/363]\n",
      "Train Loss: 0.22344. Batch: [100/363]\n",
      "Train Loss: 0.30671. Batch: [200/363]\n",
      "Train Loss: 0.38752. Batch: [300/363]\n",
      "Validation Loss: 0.34851\n",
      "Train Loss: 0.21538. Batch: [0/363]\n",
      "Train Loss: 0.26718. Batch: [100/363]\n",
      "Train Loss: 0.28972. Batch: [200/363]\n",
      "Train Loss: 0.099954. Batch: [300/363]\n",
      "Validation Loss: 0.48447\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_regression(train_loader, widedeep, loss_fn, wnd_optim)\n",
    "    eval_regression(valid_loader, widedeep, loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c82bcd13",
   "metadata": {},
   "source": [
    "### Sending inputs through two different paths\n",
    "\n",
    "The next example HOML3 sends features 0 to 4 to the wide path and features 2 to 7 through the deep path as shown below ![wide_deep](img/mls3_1015.png). To make this work we need to:\n",
    "\n",
    "1. Modify the Dataset so that it returns two inputs and one target.\n",
    "2. Modify the model so that it has two inputs.\n",
    "\n",
    "In this case, since there is only one output, the loss is unchanged. The source code for TensorDataset can be found [here](https://github.com/pytorch/pytorch/blob/03de15806e5d27ee4ef6d82dbcc66dac78f6e3bf/torch/utils/data/dataset.py#L193).\n",
    "\n",
    "We just need to redefine `__getitem__()` and we don't need to touch `__init__()`. Our modified implementation returns a tuple containing a tuple of inputs and the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be5e8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepDataset(TensorDataset):\n",
    "    def __getitem__(self, index):\n",
    "        tensor_x, tensor_y = self.tensors\n",
    "        x_wide, x_deep = tensor_x[:, :5], tensor_x[:, 2:]\n",
    "        return (x_wide[index], x_deep[index], tensor_y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8d331455",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnd_train_dset = WideAndDeepDataset(x_tensor_train, y_tensor_train)\n",
    "wnd_valid_dset = WideAndDeepDataset(x_tensor_valid, y_tensor_valid)\n",
    "wnd_test_dset = WideAndDeepDataset(x_tensor_test, y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7c4a0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnd_train_loader = DataLoader(wnd_train_dset, batch_size, shuffle=True)\n",
    "wnd_valid_loader = DataLoader(wnd_valid_dset, batch_size, shuffle=False)\n",
    "wnd_test_loader = DataLoader(wnd_test_dset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8195b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepTwoInputs(nn.Module):\n",
    "    # x_wide contains 5 features. x_deep contains 6\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = nn.Linear(in_features=6, out_features=30)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layer2 = nn.Linear(in_features=30, out_features=30)\n",
    "        self.output_layer = nn.Linear(in_features=35, out_features=1)\n",
    "    \n",
    "    def forward(self, x_wide, x_deep):\n",
    "        x_deep = self.relu(self.hidden_layer1(x_deep))\n",
    "        x_deep = self.relu(self.hidden_layer2(x_deep))\n",
    "        w = torch.cat((x_wide, x_deep), dim=-1)\n",
    "        return self.output_layer(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fee589f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two_inputs = WideAndDeepTwoInputs()\n",
    "model_two_inputs.apply(init_weights)\n",
    "model_two_inputs = model_two_inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "178fabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_inputs(dataloader, model, loss_fn, optimizer):\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (x_wide, x_deep, y) in enumerate(dataloader):\n",
    "        x_wide, x_deep, y = x_wide.to(device), x_deep.to(device), y.to(device)\n",
    "        preds = model(x_wide, x_deep)\n",
    "        batch_loss = loss_fn(preds, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Train Loss: {batch_loss.item():>.5}. Batch: [{batch}/{num_batches}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2a1801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_two_inputs(dataloader, model, loss_fn):\n",
    "    total_loss = 0\n",
    "    num_obs = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_wide, x_deep, y in dataloader:\n",
    "            x_wide, x_deep, y = x_wide.to(device), x_deep.to(device), y.to(device)\n",
    "            preds = model(x_wide, x_deep)\n",
    "            total_loss += loss_fn(preds, y).item()\n",
    "        print(f'Validation Loss: {total_loss/num_batches:>.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "77386859",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnd_optim_two_inputs = optim.Adam(widedeep.parameters(), lr=1e-3, eps=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a0fbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Epoch: 1 -----\n",
      "\n",
      "Train Loss: 6.5514. Batch: [0/363]\n",
      "Train Loss: 6.4527. Batch: [100/363]\n",
      "Train Loss: 5.63. Batch: [200/363]\n",
      "Train Loss: 12.161. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 2 -----\n",
      "\n",
      "Train Loss: 5.8491. Batch: [0/363]\n",
      "Train Loss: 7.0123. Batch: [100/363]\n",
      "Train Loss: 11.389. Batch: [200/363]\n",
      "Train Loss: 9.6632. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 3 -----\n",
      "\n",
      "Train Loss: 5.8329. Batch: [0/363]\n",
      "Train Loss: 6.0983. Batch: [100/363]\n",
      "Train Loss: 11.146. Batch: [200/363]\n",
      "Train Loss: 6.7976. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 4 -----\n",
      "\n",
      "Train Loss: 10.301. Batch: [0/363]\n",
      "Train Loss: 5.6044. Batch: [100/363]\n",
      "Train Loss: 5.8515. Batch: [200/363]\n",
      "Train Loss: 5.2471. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 5 -----\n",
      "\n",
      "Train Loss: 6.0951. Batch: [0/363]\n",
      "Train Loss: 6.3694. Batch: [100/363]\n",
      "Train Loss: 4.5136. Batch: [200/363]\n",
      "Train Loss: 11.166. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 6 -----\n",
      "\n",
      "Train Loss: 6.26. Batch: [0/363]\n",
      "Train Loss: 6.9757. Batch: [100/363]\n",
      "Train Loss: 8.0356. Batch: [200/363]\n",
      "Train Loss: 6.2546. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 7 -----\n",
      "\n",
      "Train Loss: 6.3794. Batch: [0/363]\n",
      "Train Loss: 7.8617. Batch: [100/363]\n",
      "Train Loss: 11.371. Batch: [200/363]\n",
      "Train Loss: 6.2485. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 8 -----\n",
      "\n",
      "Train Loss: 5.813. Batch: [0/363]\n",
      "Train Loss: 4.8842. Batch: [100/363]\n",
      "Train Loss: 7.7176. Batch: [200/363]\n",
      "Train Loss: 9.7811. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 9 -----\n",
      "\n",
      "Train Loss: 4.6818. Batch: [0/363]\n",
      "Train Loss: 8.0217. Batch: [100/363]\n",
      "Train Loss: 8.104. Batch: [200/363]\n",
      "Train Loss: 6.085. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 10 -----\n",
      "\n",
      "Train Loss: 6.708. Batch: [0/363]\n",
      "Train Loss: 4.8382. Batch: [100/363]\n",
      "Train Loss: 6.1185. Batch: [200/363]\n",
      "Train Loss: 6.3232. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 11 -----\n",
      "\n",
      "Train Loss: 8.0686. Batch: [0/363]\n",
      "Train Loss: 8.793. Batch: [100/363]\n",
      "Train Loss: 8.0393. Batch: [200/363]\n",
      "Train Loss: 6.2442. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 12 -----\n",
      "\n",
      "Train Loss: 6.2909. Batch: [0/363]\n",
      "Train Loss: 6.5443. Batch: [100/363]\n",
      "Train Loss: 7.5134. Batch: [200/363]\n",
      "Train Loss: 8.616. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 13 -----\n",
      "\n",
      "Train Loss: 8.186. Batch: [0/363]\n",
      "Train Loss: 5.8353. Batch: [100/363]\n",
      "Train Loss: 6.0079. Batch: [200/363]\n",
      "Train Loss: 5.9271. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 14 -----\n",
      "\n",
      "Train Loss: 4.7315. Batch: [0/363]\n",
      "Train Loss: 7.1918. Batch: [100/363]\n",
      "Train Loss: 7.4603. Batch: [200/363]\n",
      "Train Loss: 5.3764. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 15 -----\n",
      "\n",
      "Train Loss: 7.2412. Batch: [0/363]\n",
      "Train Loss: 5.5849. Batch: [100/363]\n",
      "Train Loss: 9.1383. Batch: [200/363]\n",
      "Train Loss: 9.4154. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 16 -----\n",
      "\n",
      "Train Loss: 6.231. Batch: [0/363]\n",
      "Train Loss: 5.5102. Batch: [100/363]\n",
      "Train Loss: 6.8662. Batch: [200/363]\n",
      "Train Loss: 7.5541. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 17 -----\n",
      "\n",
      "Train Loss: 10.909. Batch: [0/363]\n",
      "Train Loss: 13.614. Batch: [100/363]\n",
      "Train Loss: 7.4668. Batch: [200/363]\n",
      "Train Loss: 6.5508. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 18 -----\n",
      "\n",
      "Train Loss: 7.5181. Batch: [0/363]\n",
      "Train Loss: 6.9905. Batch: [100/363]\n",
      "Train Loss: 7.2042. Batch: [200/363]\n",
      "Train Loss: 5.2839. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 19 -----\n",
      "\n",
      "Train Loss: 5.8936. Batch: [0/363]\n",
      "Train Loss: 4.7826. Batch: [100/363]\n",
      "Train Loss: 7.8579. Batch: [200/363]\n",
      "Train Loss: 7.7851. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n",
      "\n",
      "----- Epoch: 20 -----\n",
      "\n",
      "Train Loss: 6.9109. Batch: [0/363]\n",
      "Train Loss: 8.511. Batch: [100/363]\n",
      "Train Loss: 6.9032. Batch: [200/363]\n",
      "Train Loss: 6.8721. Batch: [300/363]\n",
      "Validation Loss: 9.1545\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f\"\\n----- Epoch: {epoch+1} -----\\n\")\n",
    "    train_two_inputs(wnd_train_loader, model_two_inputs, loss_fn, wnd_optim_two_inputs)\n",
    "    eval_two_inputs(wnd_valid_loader, model_two_inputs, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb192a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
